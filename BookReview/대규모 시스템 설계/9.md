웹 크롤러 설계
=

## 규모 추정

- 매달 10억개의 웹페이지를 다운로드
- QPS = 초당 400페이지
- 월 당 500TB의 저장 용량
- 5년당 30PB 필요

## 컴포넌트 구성 요소

- 시작 URL 집합

웹 크롤러의 출발점이다.

예시로 도메인 이름이 naver인 모든 페이지의 url을 시작 url로 사용한다.

시작 url은 주제별로 다른 시작 url을 사용하는 것이 좋다.

스포츠, 건강, 쇼핑, 뉴스 등 다양한 주제를 시작 url로 사용하면 작은 부분집합들로 시작 url을 구성할 수 있다.

- 미수집 URL 저장소

크롤링의 상태는 두 가지다. 

**다운로드할 url**과 **다운로드된 url**

미수집 url은 **다운로드할 url**을 저장 관리하는 컴포넌트다.

FIFO구조로 돼있다.

- HTML 다운로더

웹페이지를 다운로드한다.

다운로드할 페이지는 미수집 url 저장소가 제공한다.

- 도메인 이름 변환기

url에 대응되는 IP주소를 알아낸다.

- 콘텐츠 파서

이상한 웹 페이지를 걸러내는 용도로 사용된다.

- 중복 컨텐츠?

29%의 웹페이지 콘텐츠는 중복이라는 연구결과가 있다.

용량 측면에서도 비효율적이고 이는 비용과도 연관된다.

HTML의 문서를 비교해야하는데 글자를 하나하나 비교하는 것은 비효율적이다.

**웹 페이지의 해시값**을 비교하는 것이 효과적이다.

- 콘텐츠 저장소

대부분은 디스크에 저장하지만 인기 있는 컨텐츠는 메모리에 둔다.(지연시간을 줄이기위해)

- URL 추출기

HTML 페이지를 파싱해 링크를 골라낸다.

상대 경로는 모두 절대 경로로 변환한다.

- URL 필터

특정한 콘텐츠나 파일 확장자를 갖는 url이나 접속 시 오류가 발생하는 url 등은 크롤링 대상에서 배제하는 역할을 한다.

- 이미 방문한 URL

블룸필터나 해시 테이블을 이용해 이미 방문한 적이 있는 url인지 추적한다.

- URL 저장소

이미 방문한 url을 보관한다.

## 작업 흐름

1. 앞서 설명했듯이 시작 url을 구성하고 미수집 url 저장소에 저장

2. HTML 다운로더는 미수집 url에서 url 목록을 가져온다.

3. url의 ip주소를 도메일 이름 변환기를 통해 알아내고 웹페이지를 다운로드한다.

4. 콘텐츠 파서를 통해 HTML다운로더가 다운로드한 HTML에서 이상한 웹페이지는 걸러내고 올바른 형식인지 검증한다.

5. 중복 컨텐츠인지 검증한다 (블룸필터 or 해시 테이블)

6. 중복이라면 버리고, 중복이 아니라면 컨텐츠 저장소에 저장하고 URL 추출기로 전달한다.

7. HTML에서 링크를 골라낸다.

8. 골라낸 링크는 URL 필터로 이동해 접속 시 오류가 발생하는지 파일확장자, 컨텐츠 타입을 걸러낸다.

9. URL필터가 골라낸 링크는 중복인지 판별한다.(URL 저장소에 저장)

10. 이미 저장된 url은 버리고 없다면 url 저장소에 저장하고 미수집 url 저장소에도 저장한다.

## 상세 설계

### DFS인가 BFS인가?

DFS는 **깊이 우선 탐색**으로 가장 깊은 노드부터 탐색을 시작한다.

BFS는 **넓이 우선 탐색**으로 한 뎁스씩 내려가며 해당 뎁스의 모든 노드를 탐색한다.

질문 : 왜 DFS는 제외인가?

### BFS로 구현 시 문제점

1. Implite

하나의 시작 페이지를 기준으로 모든 링크를 탐색한다면 결국은 같은 호스트서버로 요청을 보내게된다.

예를들어 naver라는 도메인의 모든 페이지를 BFS로 탐색한다면 결국은 네이버의 서버에 지속적으로 요청을 보내게된다.

심지어 병렬처리를 한다면 수많은 요청으로 부하가걸릴 수 있다.

이런 크롤러를 보통 **예의 없는**크롤러로 간주된다.

2. 우선순위

BFS는 모든 노드를 순차적으로 탐색한다.

즉 우선순위가 없다.

하지만 사용자가 자주 방문하고 업데이트의 빈도 등 여러 가지 척도에 우선순위를 주는 것이 맞다.

## 미수집 URL 저장소

이 컴포넌트를 사용한다면 **예의 있는**크롤러, **우선 순위를 고려한** 크롤러, **신선도를 고려한** 크롤러 설계가 가능하다.

1. 예의 있는 크롤러

해결법은 아주 단순하다.

같은 호스트에 속한 url들은 모두 같은 큐에 넣고 하나씩 처리한다.

이렇게 한다면 다른 호스트 url은 병렬처리가 가능하지만 같은 호스트url은 병렬처리가 불가능하게 만들 수 있다.

**큐 라우터**는 같은 호스트에 속한 url은 **같은 큐**에 저장되도록 보장한다.

**매핑 테이블** 호스트가 어떤 큐에 저장되어야할지를 매핑한 테이블이다.

**FIFO 큐** 같은 호스트에 속한 url은 같은 큐에 보관

**큐 선택기** 큐들을 순회하며 큐에서 url을 꺼내 작업 스레드에 할당한다.

**작업 스레드** 여러 작업스레드가 각각 순차처리를 한다.

2. 우선순위

예의 있는 크롤러의 큐들을 후면 큐로 설계하고 그 앞단에 전면 큐를 하나 더 구성한다.

url이 입력되면 순위결정장치를 통해 큐들에 삽입된다.

순위결정장치는 트래픽 양, 갱신 빈도등을 고려할 수 있다.

전면 큐의 큐 선택기가 우선 순위가 높은 큐를 더 자주 방문하도록할 수 있다.

3. 신선도

이미 수집한 웹페이지라 할 지라도 자주 변경된다면 그만큼 자주 재수집해야한다.

이는 전면 큐에서 더 자주 재수집하도록 구성할 수 있다.

## HTML 다운로더

### 로봇 제외 프로토콜

로봇 제외 프로토콜은 웹사이트와 웹 크롤러가 소통하는 표준적 방법이다.

웹사이트에는 수집해도되는 url들과 수집해서는 안되는 url들이 존재한다.

### 성능 최적화

1. 분산 크롤링

url 집합의 부분집합들을 작업스레드가 나눠서 다운로드한다.

2. 도메인 이름 변환 결과 캐시

DNS에 특정 도메인에 해당하는 IP를 요청하는 것은 보통 100ms~200ms 정도 소요된다.

더 중요한 점은 한 작업 서버가 요청을 보내면 다른 작업 서버의 요청은 모두 블록된다는 점이다.

이런 문제를 해결하기위해선 ip를 캐싱해놓고 주기적으로 dns에 요청을보내 갱신하는 방식이 있다.

3. 지역성

CDN에서도 봤듯이 지역에따라 속도의 차이는 많이 벌어진다.

서버를 지역별로 분산한다면 더 효율적인 크롤링이 가능하다.

4. 짧은 타임아웃

어떤 웹 서버는 응답을 하지않거나 매우 느릴 수 있다. 이때 타임아웃을 설정해 다음으로 빨리 넘어갈 수 있다.

### 안정성

다운로드 서버들에 부하를 분산할 때 **안정해시설계**를 사용할 수 있다.

서버를 쉽게 추가하고 삭제할 수 있다.

### 확장성

HTML이 아닌 다른 자원도 저장할 수 있도록 컴포넌트를 확장할 수 있어야한다.

PNG다운로더를 통해 모듈을 추가하는 것이 그 예시다.

### 문제 있는 콘텐츠 감지 및 회피

1. 중복 감지는 해시나 체크섬을 사용해 중복 컨텐츠를 탐지할 수 있다.

> 체크섬이란 중복을 감지하는 한 가지 형태이다.


2. 거미 덫

크롤러를 무한 루프에 빠뜨리도록 설계한 웹페이지가 있을 수 있다.

이런 덫은 url의 최대길이를 제한하여 회피할 수 있다.

3. 데이터 노이즈

필요없는 데이터는 반드시 존재한다.(스팸 url, 광고 등)

이런 컨텐츠는 제외해야한다.